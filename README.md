# ğŸ›’ SmartCart: Purchase Pattern Analytics & Customer Segmentation Pipeline

**SmartCart** is a production-grade retail analytics pipeline designed to help businesses extract insights from invoice-level data. This project enables customer segmentation, purchase behavior analysis, and KPI tracking using automated ETL workflows, machine learning models, and dynamic BI dashboards.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

## ğŸ“‹ Table of Contents
- [Business Problem](#business-problem)
- [Project Structure](#project-structure)
- [Getting Started](#getting-started)
- [Data Source & Ethics](#data-source--ethical-considerations)
- [Tech Stack](#tech-stack)
- [Contributing](#contributing)
- [License](#license)

## ğŸ¯ Business Problem

Retail and e-commerce companies often struggle to:
- Identify high-value vs. churn-prone customers
- Forecast product demand with accuracy
- Track real-time sales and operational KPIs across categories
- Automate insights without manual intervention

**SmartCart** addresses these challenges through a scalable analytics solution.

## ğŸ“ Project Structure

```
Smartcart_Retail_Analytics/
â”‚
â”œâ”€â”€ notebooks/                     # Jupyter notebooks for analysis and development
â”‚   â”œâ”€â”€ ecomm_eda.ipynb           # Main exploratory data analysis notebook
â”‚   â””â”€â”€ model_dev.ipynb           # Model development and evaluation notebook
â”‚
â”œâ”€â”€ src/                           # Source code for the project
â”‚   â”œâ”€â”€ dags/                     # Airflow DAGs for ETL workflows
â”‚   â”‚   â””â”€â”€ transform_load.py     # Data transformation and loading logic
â”‚   â””â”€â”€ config/                   # Configuration files
â”‚
â”œâ”€â”€ terraform/                     # Infrastructure as Code (IaC) for GCP
â”‚   â””â”€â”€ gcp/                      # GCP-specific configurations
â”‚       â””â”€â”€ main.tf               # Main Terraform configuration
â”‚
â”œâ”€â”€ data/                          # Data storage (not versioned)
â”‚   â””â”€â”€ ecomm_invoice_transaction.parquet  # Processed data file
â”‚
â”œâ”€â”€ lib/                           # External dependencies
â”‚   â””â”€â”€ gcs-connector-3.0.7-shaded.jar  # Google Cloud Storage connector for Spark
â”‚
â”œâ”€â”€ sandbox/                       # Experimental code and tests
â”‚   â””â”€â”€ pyspark_transformation_test.ipynb  # PySpark experimentation
â”‚
â”œâ”€â”€ tests/                         # Test files
â”œâ”€â”€ docs/                          # Project documentation
â”œâ”€â”€ web-scraping/                  # Web scraping utilities and scripts
â”œâ”€â”€ docker/                        # Docker container configurations
â””â”€â”€ credentials/                   # Store for credential files (in .gitignore)
```

### Key Files and Their Purposes

#### Core Analysis Files
- `notebooks/ecomm_eda.ipynb`: Main notebook for exploratory data analysis
- `notebooks/model_dev.ipynb`: Notebook for model development and evaluation
- `sandbox/pyspark_transformation_test.ipynb`: Experimental PySpark code and tests

#### Data Pipeline
- `src/dags/transform_load.py`: ETL pipeline for data transformation and loading
- `src/config/`: Configuration files for the data pipeline

#### Infrastructure
- `terraform/gcp/main.tf`: GCP infrastructure configuration using Terraform
- `docker/`: Docker container configurations for deployment

#### Data
- `data/ecomm_invoice_transaction.parquet`: Processed dataset used for analysis

#### Dependencies
- `requirements.txt`: Python package dependencies
- `pyproject.toml`: Python project configuration (for Poetry)
- `poetry.lock`: Lock file for Poetry dependency management

#### Configuration
- `.gitignore`: Specifies intentionally untracked files to ignore
- `.pre-commit-config.yaml`: Pre-commit hooks configuration
- `ruff.toml`: Configuration for the Ruff linter

## ğŸš€ Getting Started

### Prerequisites
- Python 3.8+
- Jupyter Notebook
- Required Python packages (see `requirements.txt`)

### Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/smartcart-retail-analytics.git
   cd smartcart-retail-analytics
   ```

2. Create and activate a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

### Running the Analysis
1. Launch Jupyter Notebook:
   ```bash
   jupyter notebook analysis/notebooks/
   ```
2. Open `01_eda.ipynb` to start with exploratory data analysis

## ğŸ” Data Source & Ethical Considerations

This project uses anonymized e-commerce transaction data for educational and demonstration purposes. Key points to note:

- **Data Anonymization**: All personally identifiable information has been removed
- **Ethical Web Scraping**: Data collection follows ethical guidelines including:
  - Respect for robots.txt rules
  - Rate-limited requests
  - No collection of personal or sensitive information
- **Purpose**: Strictly for educational and portfolio demonstration
- **Attribution**: Proper attribution given to data sources in compliance with terms of use

For detailed scraping methodology and ethical considerations, see the [web-scraping/README.md](web-scraping/README.md)

## ğŸ›  Tech Stack

| Layer            | Tools                            |
|------------------|----------------------------------|
| **Programming**  | Python, SQL                      |
| **Data Processing** | Pandas, NumPy, PySpark         |
| **BI Dashboards**| Power BI, Tableau                |
| **Data Modeling**| Scikit-learn, Statsmodels        |
| **Automation**   | Airflow, Docker                  |
| **Cloud**        | GCP, AWS                         |
| **Version Control** | Git, GitHub                    |

## ğŸ¤ Contributing

Contributions are welcome! Please read our [contributing guidelines](CONTRIBUTING.md) before submitting pull requests.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.


---

## Key Features

- âœ… Customer Segmentation using RFM scoring and clustering
- âœ… Demand Forecasting using time series models
- âœ… Real-time Dashboards for Sales & Customer KPIs
- âœ… Automated ETL Pipelines for data ingestion and transformation
- âœ… Clean modular architecture ready for MLOps integration

---

## Folder Structure

```text
smartcart-retail-analytics/
â”œâ”€â”€ code/                  # Data transformation scripts
â”œâ”€â”€ notebooks/             # EDA, modeling, segmentation notebooks
â”œâ”€â”€ data/                  # Raw & cleaned invoice datasets
â”œâ”€â”€ docker/                # Docker configuration (in progress)
â”œâ”€â”€ docs/                  # Supporting documentation
â”œâ”€â”€ credentials/           # Placeholder for keys (not tracked)
â”œâ”€â”€ src/                   # Python modules for models, pipelines
â”œâ”€â”€ terraform/             # IaC for cloud infra (planned)
â”œâ”€â”€ tests/                 # Unit tests (in progress)
â”œâ”€â”€ web-scraping/          # Scraping scripts for dynamic data
â”œâ”€â”€ ecomm_bi.pptx          # Power BI dashboard file
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # Youâ€™re here.

```

---

## Key Deliverables

- Designed the complete analytics pipeline from raw invoice data to automated customer segmentation and KPI dashboards  
- Developed predictive models and RFM-based segmentation logic for demand and retention optimization  
- Automated ETL and reporting workflows using Python and SQL; deployed visual insights via Power BI  
- Structured the codebase for future scaling with MLOps and cloud integration in mind


---

## Project Status

- âœ… Invoice data analysis, segmentation, dashboarding â€“ **Complete**
- ğŸ› ï¸ MLOps components (Airflow, Docker) â€“ Implemented pipeline structure; refining for production-readiness
- â˜ï¸ Cloud deployment (GCP, AWS) â€“ Repository structured for cloud; deployment steps to be documented

---

## How to Use
1. Clone the repo

2. Install dependencies:
```text
pip install -r requirements.txt
```
3. Explore EDA and modeling in the notebooks/ directory

4. Open ecomm_bi.pptx to view Power BI dashboard

5. Dashboard automation via Airflow â†’ coming soon

